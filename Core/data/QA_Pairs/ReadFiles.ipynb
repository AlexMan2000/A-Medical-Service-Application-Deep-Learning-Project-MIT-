{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_json(\"./icliniqQAs.json\") #465\n",
    "dataset2 = pd.read_json(\"./questionDoctorQAs.json\") #5679\n",
    "dataset3 = pd.read_json(\"./ehealthforumQAs.json\") #171\n",
    "dataset4 = pd.read_json(\"./webmdQAs.json\") # 23437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1890"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list = list(dict(dataset2.question.value_counts()).keys())\n",
    "len(questions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf                                                                                                                                                           \n",
    "import keras                                                                                                         \n",
    "from gensim.models import KeyedVectors                                                                               \n",
    "from keras import initializers as initializers, regularizers, constraints                                            \n",
    "from keras.models import Model                                                                                       \n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Flatten, Activation, RepeatVector, Permute, Lambda, Bidirectional, TimeDistributed, Dropout, Conv1D, GlobalMaxPool1D                                                 \n",
    "from keras.layers.merge import multiply, concatenate                                                                 \n",
    "import keras.backend as K                                                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "'''\n",
    "本配置文件提供了一系列预定义函数\n",
    "'''\n",
    "\n",
    "\n",
    "# ------------------自定义函数------------------ #\n",
    "\n",
    "def text_to_word_list(flag, text):  # 文本分词\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    if flag == 'cn':\n",
    "        pass\n",
    "    else:\n",
    "        # 英文文本下的文本清理规则\n",
    "        import re\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" ! \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "        text = re.sub(r\"\\+\", \" + \", text)\n",
    "        text = re.sub(r\"\\-\", \" - \", text)\n",
    "        text = re.sub(r\"\\=\", \" = \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "        text = re.sub(r\":\", \" : \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        text = re.sub(r\"\\0s\", \"0\", text)\n",
    "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "        text = re.sub(r\"e - mail\", \"email\", text)\n",
    "        text = re.sub(r\"j k\", \"jk\", text)\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def make_w2v_embeddings(flag, word2vec, df, embedding_dim):  # 将词转化为词向量\n",
    "    vocabs = {}  # 词序号\n",
    "    vocabs_cnt = 0  # 词个数计数器\n",
    "\n",
    "    vocabs_not_w2v = {}  # 无法用词向量表示的词\n",
    "    vocabs_not_w2v_cnt = 0  # 无法用词向量表示的词个数计数器\n",
    "\n",
    "    # 停用词\n",
    "    # stops = set(open('data/stopwords.txt').read().strip().split('\\n'))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # 打印处理进度\n",
    "        if index != 0 and index % 1000 == 0:\n",
    "            print(str(index) + \" sentences embedded.\")\n",
    "\n",
    "        for question in ['question1', 'question2']:\n",
    "            q2n = []  # q2n -> question to numbers representation\n",
    "            words = text_to_word_list(flag, row[question])\n",
    "\n",
    "            for word in words:\n",
    "                # if word in stops:  # 去停用词\n",
    "                    # continue\n",
    "                if word not in word2vec and word not in vocabs_not_w2v:  # OOV的词放入不能用词向量表示的字典中，value为1\n",
    "                    vocabs_not_w2v_cnt += 1\n",
    "                    vocabs_not_w2v[word] = 1\n",
    "                if word not in vocabs:  # 非OOV词，提取出对应的id\n",
    "                    vocabs_cnt += 1\n",
    "                    vocabs[word] = vocabs_cnt\n",
    "                    q2n.append(vocabs_cnt)\n",
    "                else:\n",
    "                    q2n.append(vocabs[word])\n",
    "            df.at[index, question + '_n'] = q2n\n",
    "\n",
    "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # 随机初始化一个形状为[全部词个数，词向量维度]的矩阵\n",
    "    '''\n",
    "    词1 [a1, a2, a3, ..., a60]\n",
    "    词2 [b1, b2, b3, ..., b60]\n",
    "    词3 [c1, c2, c3, ..., c60]\n",
    "    '''\n",
    "    embeddings[0] = 0  # 第一行用0填充，因为不存在index为0的词\n",
    "\n",
    "    for word,index in vocabs.items():\n",
    "        if word in word2vec:\n",
    "            embeddings[index] = word2vec[word]\n",
    "    del word2vec\n",
    "    return df, embeddings,vocabs\n",
    "\n",
    "\n",
    "def split_and_zero_padding(df, max_seq_length):  # 调整tokens长度\n",
    "    print(df[\"question1_n\"])\n",
    "    print(df[\"question2_n\"])\n",
    "    # 训练集矩阵转换成字典\n",
    "    X = {'left': df['question1_n'], 'right': df['question2_n']}\n",
    "\n",
    "    # 调整到规定长度\n",
    "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
    "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ManDist(Layer):  # 封装成keras层的曼哈顿距离计算\n",
    "\n",
    "    # 初始化ManDist层，此时不需要任何参数输入\n",
    "    def __init__(self, **kwargs):\n",
    "        self.result = None\n",
    "        super(ManDist, self).__init__(**kwargs)\n",
    "\n",
    "    # 自动建立ManDist层\n",
    "    def build(self, input_shape):\n",
    "        super(ManDist, self).build(input_shape)\n",
    "\n",
    "    # 计算曼哈顿距离\n",
    "    def call(self, x, **kwargs):\n",
    "        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n",
    "        return self.result\n",
    "\n",
    "    # 返回结果\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(** kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        # general\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W)))\n",
    "        a = K.permute_dimensions(a, (0, 2, 1))\n",
    "        outputs = a * inputs\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model(it may takes 2-3 mins) ...\n",
      "1000 sentences embedded.\n",
      "2000 sentences embedded.\n",
      "3000 sentences embedded.\n",
      "4000 sentences embedded.\n",
      "5000 sentences embedded.\n",
      "6000 sentences embedded.\n",
      "7000 sentences embedded.\n",
      "8000 sentences embedded.\n",
      "9000 sentences embedded.\n",
      "4488    [19, 163, 9, 164, 62, 896, 402, 1, 2, 26, 487,...\n",
      "9509         [1, 25, 15, 68, 26, 401, 157, 816, 62, 2361]\n",
      "4219    [274, 25, 15, 68, 619, 74, 156, 6, 278, 62, 24...\n",
      "2679    [122, 143, 156, 2923, 15, 1538, 1539, 1540, 2924]\n",
      "5992       [17, 18, 171, 364, 3122, 56, 2663, 9328, 9329]\n",
      "                              ...                        \n",
      "2819                          [2, 2754, 2593, 3644, 5872]\n",
      "5806    [21, 673, 361, 9400, 229, 251, 1784, 21, 3907,...\n",
      "9846    [850, 9, 311, 931, 208, 3266, 229, 19, 11658, ...\n",
      "9000                         [17, 18, 19, 265, 74, 10403]\n",
      "8767    [17, 65, 789, 790, 791, 66, 67, 217, 54, 25, 9...\n",
      "Name: question1_n, Length: 7998, dtype: object\n",
      "4488    [1, 2, 15, 26, 487, 712, 224, 19, 611, 154, 54...\n",
      "9509                       [1, 25, 9, 26, 401, 157, 2361]\n",
      "4219           [274, 25, 15, 68, 2455, 278, 619, 74, 156]\n",
      "2679              [122, 143, 156, 4811, 1538, 1539, 1540]\n",
      "5992    [17, 18, 115, 3122, 407, 2663, 4214, 286, 641,...\n",
      "                              ...                        \n",
      "2819                         [25, 2754, 2593, 2694, 1640]\n",
      "5806        [1, 2, 15, 327, 142, 65, 677, 1266, 152, 261]\n",
      "9846    [9, 311, 449, 19, 719, 74, 476, 2973, 8342, 22...\n",
      "9000                         [17, 51, 19, 265, 74, 10403]\n",
      "8767                    [17, 18, 789, 790, 791, 66, 9312]\n",
      "Name: question2_n, Length: 7998, dtype: object\n",
      "626     [51, 3, 138, 96, 1977, 218, 51, 221, 2359, 22,...\n",
      "3935                         [17, 18, 19, 1724, 28, 4809]\n",
      "5447                    [17, 51, 19, 767, 265, 222, 1218]\n",
      "7784    [25, 54, 1640, 11, 26, 74, 545, 2396, 6651, 62...\n",
      "3846    [236, 153, 596, 51, 19, 361, 267, 56, 127, 1, ...\n",
      "                              ...                        \n",
      "9924                       [274, 2, 15, 68, 1198, 6, 930]\n",
      "3512          [1, 2, 15, 64, 62, 749, 265, 981, 1202, 16]\n",
      "3635    [1, 25, 15, 1252, 850, 11, 9, 321, 65, 265, 74...\n",
      "8144                   [25, 358, 248, 278, 6, 6038, 6039]\n",
      "5355    [1, 25, 15, 68, 489, 52, 4342, 56, 1648, 672, 56]\n",
      "Name: question1_n, Length: 2000, dtype: object\n",
      "626           [1, 2, 15, 1258, 138, 51, 221, 88, 56, 757]\n",
      "3935                         [17, 51, 19, 1724, 28, 4809]\n",
      "5447                     [17, 18, 19, 767, 223, 14, 1218]\n",
      "7784    [25, 54, 1640, 11, 15, 26, 74, 545, 2396, 9, 3...\n",
      "3846    [17, 51, 19, 221, 127, 1, 457, 138, 129, 62, 229]\n",
      "                              ...                        \n",
      "9924                [1, 2, 15, 68, 147, 1233, 26, 6, 930]\n",
      "3512                   [1, 2, 15, 64, 62, 5241, 265, 981]\n",
      "3635    [5028, 1, 25, 15, 2452, 1252, 43, 755, 151, 18...\n",
      "8144                              [25, 6038, 6039, 10378]\n",
      "5355             [1, 25, 15, 68, 4342, 56, 1648, 672, 56]\n",
      "Name: question2_n, Length: 2000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                   \n",
    "# ------------------预加载------------------ #                                                                                                                                                                                                            \n",
    "TRAIN_CSV = r'C:\\Users\\DELL\\Desktop\\MIT_Final_Project\\Core\\data\\Train_Test_Data\\train.csv'                      \n",
    "flag = 'en'                                                                                                     \n",
    "embedding_path = r'F:\\Downloads\\GoogleNews-vectors-negative300.bin.gz'                                          \n",
    "embedding_dim = 300                                                                                             \n",
    "max_seq_length = 10                                                                                                                                                                                                                                                                                 \n",
    "                                                                                                                \n",
    "# 加载词向量                                                                                                         \n",
    "print(\"Loading word2vec model(it may takes 2-3 mins) ...\")                                                      \n",
    "embedding_dict = KeyedVectors.load_word2vec_format(embedding_path, binary=True)                                 \n",
    "                                                                                                                \n",
    "# 读取并加载训练集                                                                                                      \n",
    "train_df = pd.read_csv(TRAIN_CSV,encoding = 'gb18030')                                                          \n",
    "for q in ['question1', 'question2']:                                                                            \n",
    "    train_df[q + '_n'] = train_df[q]                                                                            \n",
    "                                                                                                                \n",
    "# 将训练集词向量化                                                                                                      \n",
    "train_df, embeddings, vocabs = make_w2v_embeddings(flag, embedding_dict, train_df, embedding_dim=embedding_dim) \n",
    "# print(vocabs)                                                                                                 \n",
    "# print(len(vocabs))                                                                                            \n",
    "# print(embeddings)                                                                                             \n",
    "# print(embeddings.shape)                                                                                       \n",
    "# print(train_df)                                                                                                            \n",
    "\n",
    "\n",
    "# X_input = input(\"You:\")\n",
    "# for question in questions_list:\n",
    "    \n",
    "    \n",
    "\n",
    "# # 分割训练集                                                                                                              \n",
    "X = train_df[['question1_n', 'question2_n']]                                                                         \n",
    "Y = train_df['is_duplicate']                                                                                         \n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.2)                                 \n",
    "X_train = split_and_zero_padding(X_train, max_seq_length)                                                            \n",
    "X_validation = split_and_zero_padding(X_validation, max_seq_length)                                                  \n",
    "                                                                                                                     \n",
    "# 将标签转化为数值                                                                                                           \n",
    "# Y_train = Y_train.values                                                                                             \n",
    "# Y_validation = Y_validation.values                                                                                   \n",
    "                                                                                                                     \n",
    "# # 确认数据准备完毕且正确                                                                                                        \n",
    "# assert X_train['left'].shape == X_train['right'].shape                                                               \n",
    "# assert len(X_train['left']) == len(Y_train)                                                                          \n",
    "# print(\"hha\")                                                                                                         \n",
    "# -----------------基础函数------------------ #                                                                          \n",
    "                                                                                                                     \n",
    "                                                                        \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 10, 300)      3514800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 300)      3514800     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 10, 200)      320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 10, 200)      320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 10, 200)      40200       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 10, 200)      40200       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer (None, 200)          100         time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_1 (AttentionLay (None, 200)          100         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "man_dist (ManDist)              (None, 1)            0           attention_layer[0][0]            \n",
      "                                                                 attention_layer_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 401)          0           attention_layer[0][0]            \n",
      "                                                                 attention_layer_1[0][0]          \n",
      "                                                                 man_dist[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           6432        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            68          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            10          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            3           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,758,313\n",
      "Trainable params: 7,758,313\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def shared_model_HBDA(_input):                                                                                                                                                       \n",
    "    embedding_layer = Embedding(len(embeddings) + 1,                                                                 \n",
    "                            embedding_dim,                                                                           \n",
    "                            input_length=max_seq_length)                                                             \n",
    "    embedded_sequences = embedding_layer(_input)                                                                     \n",
    "    l_lstm = Bidirectional(LSTM(100, return_sequences=True))(embedded_sequences)                                     \n",
    "    l_dense = TimeDistributed(Dense(200))(l_lstm)                                                                    \n",
    "    l_att = AttentionLayer()(l_dense)                                                                                \n",
    "                                                   \n",
    "                                                                                                                  \n",
    "    return l_att          \n",
    "\n",
    "# 超参                                                                                                             \n",
    "batch_size = 1024                                                                                                \n",
    "n_epoch = 9                                                                                                      \n",
    "n_hidden = 50                                                                                                    \n",
    "\n",
    "left_input = Input(shape=(max_seq_length,), dtype='float32')                                                     \n",
    "right_input = Input(shape=(max_seq_length,), dtype='float32')                                                    \n",
    "left_sen_representation = shared_model_HBDA(left_input)                                                          \n",
    "right_sen_representation = shared_model_HBDA(right_input)                                                        \n",
    "                                    \n",
    "man_distance = ManDist()([left_sen_representation, right_sen_representation])                                    \n",
    "sen_representation = concatenate([left_sen_representation, right_sen_representation, man_distance])              \n",
    "similarity = Dense(1, activation='sigmoid')(Dense(2)(Dense(4)(Dense(16)(sen_representation))))                   \n",
    "model = Model(inputs=[left_input, right_input], outputs=[similarity])                                            \n",
    "\n",
    "model.load_weights(\"./en_SiameseLSTM.h5\")              \n",
    "model.summary()                                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_zero_padding(df, max_seq_length):  # 调整tokens长度\n",
    "    # 训练集矩阵转换成字典\n",
    "    X = {'left': df['question1_n'], 'right': df['question2_n']}\n",
    "\n",
    "    # 调整到规定长度\n",
    "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
    "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the medical QA bot, press q to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What is fever?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medic: this seems to be a chronic disease like tuberculosis brucellosis. get a chest physician opinion. akt will help cure the disease if tuberculosis is confirmed.\n",
      "Medic: https://questiondoctors.com/i-have-night-sweats-cant-focus-lack-of-sleep-weight-loss-what-is-going-on/\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: what is tuberculosis?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medic: if you are looking for natural methods i am sorry to say there is nothing safe 100% you can use condoms as a method but it does not guarantee of course to prevent conceiving. about the period regulation you can do exercise yoga and meditation to help regulate them as irregular periods are sometimes due to nervousness and stress.\n",
      "Medic: https://questiondoctors.com/i-really-want-to-start-having-children-but-dont-want-to-rely-on-birth-control-to-regulate-my-cycle/\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: How to treat lung cancer?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medic: if you are looking for natural methods i am sorry to say there is nothing safe 100% you can use condoms as a method but it does not guarantee of course to prevent conceiving. about the period regulation you can do exercise yoga and meditation to help regulate them as irregular periods are sometimes due to nervousness and stress.\n",
      "Medic: https://questiondoctors.com/i-really-want-to-start-having-children-but-dont-want-to-rely-on-birth-control-to-regulate-my-cycle/\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What is skin cancer?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medic: in my opinion first check for tuberculosis if it comes back negative then i recommend you try atarax 10 mg twice a day if it helps then i recommend you see a psychologist good luck\n",
      "Medic: https://questiondoctors.com/i-have-night-sweats-cant-focus-lack-of-sleep-weight-loss-what-is-going-on/\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def word_to_index(words):\n",
    "    embedded = [0]*len(words)\n",
    "    \n",
    "    for index,word in enumerate(words):\n",
    "        \n",
    "        try:\n",
    "            word_index = vocabs[word]\n",
    "        except:\n",
    "            word_index = vocabs[\"what\"]\n",
    "        embedded[index] = word_index\n",
    "        \n",
    "    return np.array(embedded)\n",
    "\n",
    "print(\"Welcome to the medical QA bot, press q to quit\")\n",
    "quit = False\n",
    "while not quit:\n",
    "    \n",
    "    \n",
    "    X_input = input(\"User:\")\n",
    "    if X_input==\"q\":\n",
    "        print(\"Goodbye!\");\n",
    "        break;\n",
    "    words = text_to_word_list(flag, X_input)\n",
    "    input_seq = word_to_index(words)\n",
    "\n",
    "    count = 0\n",
    "    prob_list = []\n",
    "    for index,question in enumerate(questions_list):\n",
    "        if count < 100:\n",
    "            words_list = text_to_word_list(flag, question)\n",
    "            compare = word_to_index(words_list)\n",
    "            temp_df = pd.DataFrame({\"question1_n\":[0],\"question2_n\":[0]},dtype=\"object\")\n",
    "            temp_df.at[0,\"question1_n\"] = input_seq\n",
    "            temp_df.at[0,\"question2_n\"] = compare\n",
    "            temp = split_and_zero_padding(temp_df,10)\n",
    "            result = model([temp[\"left\"],temp[\"right\"]])\n",
    "            prob_list.append((index,result.numpy()[0][0]))\n",
    "    #         print(result.numpy()[0][0])\n",
    "        count+=1\n",
    "    prob_list.sort(key = lambda x:x[1],reverse=True)\n",
    "    \n",
    "    import random\n",
    "    top1 = prob_list[:1]\n",
    "    answer_list = []\n",
    "    url_list = []\n",
    "    index = 0\n",
    "    for candidate in top1:\n",
    "        question_index = candidate[0]\n",
    "        question_sentences = list(dataset2[dataset2.question==questions_list[question_index]].answer)\n",
    "        url_reference = list(dataset2[dataset2.question==questions_list[question_index]].url)\n",
    "        answer_list.extend(question_sentences)\n",
    "        url_list.extend(url_reference)\n",
    "\n",
    "    randindex = random.randint(0,len(answer_list)-1)\n",
    "    randindex_url = random.randint(0,len(url_list)-1)\n",
    "    answer = answer_list[randindex]\n",
    "    url = url_list[randindex_url]\n",
    "    \n",
    "    \n",
    "    print(\"Medic:\",answer)\n",
    "    print(\"Medic:\",url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://questiondoctors.com/im-feeling-discomfort-in-chest-i-did-ecg-and-chest-x-ray-all-normal/'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
